"""
Adaptive Vector Optimizer Router - Dynamic Vector Search Optimization
Automatically tunes vector search parameters for optimal performance
"""

import time
import statistics
from typing import Optional, List, Dict, Any
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from database import get_db_connection
from error_factory import ErrorFactory

router = APIRouter()


class VectorSearchRequest(BaseModel):
    embedding: List[float]
    distance_metric: str = "cosine"
    limit: int = 10
    threshold: float = 0.7
    auto_tune: bool = True
    database: str = "finops_auditor"


class VectorSearchResult(BaseModel):
    source_id: str
    content: str
    distance: float
    relevance_score: float


class OptimizationMetrics(BaseModel):
    search_time_ms: float
    results_count: int
    avg_distance: float
    distribution_quality: float
    parameter_efficiency: float


class VectorSearchResponse(BaseModel):
    success: bool
    results: List[VectorSearchResult]
    optimized_params: Dict[str, Any]
    metrics: OptimizationMetrics
    performance_gain: Optional[str] = None
    recommendations: List[str]


class VectorDistributionRequest(BaseModel):
    database: str = "finops_auditor"
    sample_size: int = 1000


class VectorDistributionResponse(BaseModel):
    success: bool
    total_vectors: int
    dimension: int
    statistics: Dict[str, float]
    recommended_threshold: float
    recommended_limit: int


# Cache for distribution statistics (in production, use Redis)
_distribution_cache = {
    "last_update": None,
    "statistics": None,
    "recommended_threshold": 0.7,
    "recommended_limit": 10
}


def calculate_distribution_quality(distances: List[float]) -> float:
    """
    Calculate quality score based on distance distribution
    
    Good distribution: Results spread across distance range
    Poor distribution: All results clustered at same distance
    """
    if len(distances) < 2:
        return 0.5
    
    try:
        std_dev = statistics.stdev(distances)
        mean_dist = statistics.mean(distances)
        
        # Coefficient of variation (normalized std dev)
        cv = std_dev / mean_dist if mean_dist > 0 else 0
        
        # Quality score: higher CV = better distribution
        # Scale to 0-1 range
        quality = min(cv * 2, 1.0)
        
        return quality
    except:
        return 0.5


def calculate_parameter_efficiency(
    results_count: int,
    limit: int,
    avg_distance: float,
    threshold: float
) -> float:
    """
    Calculate how efficiently parameters are set
    
    Efficient: Getting close to limit without hitting it
    Inefficient: Getting way fewer results than limit (threshold too strict)
                or hitting limit exactly (might be missing good results)
    """
    
    if limit == 0:
        return 0.0
    
    # Ideal fill rate: 70-90% of limit
    fill_rate = results_count / limit
    
    if 0.7 <= fill_rate <= 0.9:
        efficiency = 1.0
    elif fill_rate < 0.7:
        # Too few results - threshold might be too strict
        efficiency = fill_rate / 0.7
    else:
        # Hitting limit - might need higher limit
        efficiency = 0.9
    
    # Factor in threshold utilization
    threshold_utilization = avg_distance / threshold if threshold > 0 else 0
    
    # Combine metrics
    combined_efficiency = (efficiency + min(threshold_utilization, 1.0)) / 2
    
    return combined_efficiency


def optimize_search_parameters(
    initial_results: List[Dict],
    initial_threshold: float,
    initial_limit: int,
    distribution_stats: Optional[Dict] = None
) -> Dict[str, Any]:
    """
    Dynamically optimize search parameters based on results
    
    Returns optimized threshold and limit
    """
    
    if not initial_results:
        # No results - relax threshold
        return {
            "threshold": min(initial_threshold * 1.5, 1.0),
            "limit": initial_limit,
            "reason": "No results found - relaxed threshold"
        }
    
    distances = [r.get("distance", 1.0) for r in initial_results]
    avg_distance = statistics.mean(distances) if distances else 0.5
    
    # Use distribution statistics if available
    if distribution_stats:
        recommended_threshold = distribution_stats.get("recommended_threshold", 0.7)
        recommended_limit = distribution_stats.get("recommended_limit", 10)
    else:
        recommended_threshold = initial_threshold
        recommended_limit = initial_limit
    
    optimized_threshold = initial_threshold
    optimized_limit = initial_limit
    reason = "Parameters already optimal"
    
    # Optimization logic
    if len(initial_results) < initial_limit * 0.5:
        # Too few results - relax threshold
        optimized_threshold = min(avg_distance * 1.3, 1.0)
        reason = "Increased threshold to get more results"
    elif len(initial_results) >= initial_limit:
        # Hitting limit - might need more
        optimized_limit = int(initial_limit * 1.5)
        reason = "Increased limit to capture more relevant results"
    
    # Use distribution-based recommendations if available
    if distribution_stats and abs(optimized_threshold - recommended_threshold) > 0.1:
        optimized_threshold = (optimized_threshold + recommended_threshold) / 2
        reason += " (adjusted based on corpus distribution)"
    
    return {
        "threshold": round(optimized_threshold, 3),
        "limit": optimized_limit,
        "reason": reason
    }


def generate_recommendations(
    metrics: OptimizationMetrics,
    optimized_params: Dict
) -> List[str]:
    """Generate actionable recommendations"""
    
    recommendations = []
    
    if metrics.search_time_ms > 100:
        recommendations.append("‚ö° Consider adding vector index for faster search")
        recommendations.append("üí° Reduce dimension size if possible (e.g., 384 ‚Üí 256)")
    
    if metrics.distribution_quality < 0.3:
        recommendations.append("‚ö†Ô∏è Poor result distribution - consider retraining embeddings")
    
    if metrics.parameter_efficiency < 0.5:
        recommendations.append(f"üîß Adjust threshold to {optimized_params['threshold']}")
        recommendations.append(f"üìä Adjust limit to {optimized_params['limit']}")
    
    if metrics.avg_distance > 0.8:
        recommendations.append("‚ö†Ô∏è High average distance - results may not be relevant")
        recommendations.append("üí° Consider using different embedding model")
    
    if not recommendations:
        recommendations.append("‚úÖ Vector search parameters are well-optimized")
    
    return recommendations


@router.post("/vector/optimize-search", response_model=VectorSearchResponse)
async def optimize_vector_search(request: VectorSearchRequest):
    """
    üéØ Adaptive Vector Search Optimizer
    
    Automatically tunes vector search parameters for optimal performance:
    - Dynamic threshold adjustment based on result distribution
    - Adaptive limit sizing
    - Performance monitoring
    - Distribution quality analysis
    
    Perfect for MariaDB Vector 11.7+ with HNSW indexes.
    
    Example:
    ```json
    {
      "embedding": [0.1, 0.2, ...],
      "distance_metric": "cosine",
      "auto_tune": true,
      "limit": 10,
      "threshold": 0.7
    }
    ```
    """
    
    start_time = time.time()
    
    # Validate embedding dimension
    if len(request.embedding) not in [256, 384, 512, 768, 1024, 1536]:
        raise HTTPException(
            status_code=400,
            detail=f"Unsupported embedding dimension: {len(request.embedding)}. Supported: 256, 384, 512, 768, 1024, 1536"
        )
    
    # Validate distance metric
    if request.distance_metric not in ["cosine", "euclidean", "dot"]:
        raise HTTPException(
            status_code=400,
            detail=f"Unsupported distance metric: {request.distance_metric}. Supported: cosine, euclidean, dot"
        )
    
    try:
        conn = get_db_connection(database=request.database)
        cursor = conn.cursor(dictionary=True)
        
        # Convert embedding to string format
        embedding_str = str(request.embedding)
        
        # Select distance function based on metric
        if request.distance_metric == "cosine":
            distance_func = "VEC_DISTANCE_COSINE"
        elif request.distance_metric == "euclidean":
            distance_func = "VEC_DISTANCE_EUCLIDEAN"
        else:  # dot
            distance_func = "VEC_DISTANCE_DOT"
        
        # Execute search
        search_start = time.time()
        
        query = f"""
            SELECT 
                source_id,
                content,
                {distance_func}(VEC_FromText(?), embedding) as distance
            FROM doc_embeddings
            WHERE {distance_func}(VEC_FromText(?), embedding) < ?
            ORDER BY distance ASC
            LIMIT ?
        """
        
        cursor.execute(query, (
            embedding_str,
            embedding_str,
            request.threshold,
            request.limit
        ))
        
        raw_results = cursor.fetchall()
        search_time_ms = round((time.time() - search_start) * 1000, 2)
        
        cursor.close()
        conn.close()
        
        # Process results
        results = []
        distances = []
        
        for row in raw_results:
            distance = float(row.get("distance", 1.0))
            distances.append(distance)
            
            # Convert distance to relevance score (0-100)
            relevance_score = round((1 - distance) * 100, 1)
            
            results.append(VectorSearchResult(
                source_id=row.get("source_id", "unknown"),
                content=row.get("content", "")[:500],
                distance=distance,
                relevance_score=relevance_score
            ))
        
        # Calculate metrics
        avg_distance = statistics.mean(distances) if distances else 0.0
        distribution_quality = calculate_distribution_quality(distances)
        parameter_efficiency = calculate_parameter_efficiency(
            len(results),
            request.limit,
            avg_distance,
            request.threshold
        )
        
        metrics = OptimizationMetrics(
            search_time_ms=search_time_ms,
            results_count=len(results),
            avg_distance=round(avg_distance, 3),
            distribution_quality=round(distribution_quality, 3),
            parameter_efficiency=round(parameter_efficiency, 3)
        )
        
        # Optimize parameters if auto_tune enabled
        optimized_params = {
            "threshold": request.threshold,
            "limit": request.limit,
            "reason": "Auto-tune disabled"
        }
        
        performance_gain = None
        
        if request.auto_tune:
            optimized_params = optimize_search_parameters(
                raw_results,
                request.threshold,
                request.limit,
                _distribution_cache.get("statistics")
            )
            
            # Calculate performance gain
            if optimized_params["threshold"] != request.threshold or optimized_params["limit"] != request.limit:
                efficiency_gain = (parameter_efficiency - 0.5) * 100
                performance_gain = f"{abs(int(efficiency_gain))}% {'improvement' if efficiency_gain > 0 else 'potential'} with optimized params"
        
        # Generate recommendations
        recommendations = generate_recommendations(metrics, optimized_params)
        
        total_time_ms = round((time.time() - start_time) * 1000, 2)
        
        return VectorSearchResponse(
            success=True,
            results=results,
            optimized_params=optimized_params,
            metrics=metrics,
            performance_gain=performance_gain,
            recommendations=recommendations
        )
        
    except Exception as e:
        service_error = ErrorFactory.service_error(
            "Vector Search Optimizer",
            f"Failed to execute optimized vector search on database {request.database}",
            original_error=e
        )
        raise HTTPException(
            status_code=500,
            detail=str(service_error)
        )


@router.post("/vector/analyze-distribution", response_model=VectorDistributionResponse)
async def analyze_vector_distribution(request: VectorDistributionRequest):
    """
    üìä Analyze Vector Distribution
    
    Analyzes the distribution of vectors in the database to recommend
    optimal search parameters.
    
    This should be run periodically (e.g., daily) to keep recommendations fresh.
    """
    
    try:
        conn = get_db_connection(database=request.database)
        cursor = conn.cursor(dictionary=True)
        
        # Get total count
        cursor.execute("SELECT COUNT(*) as total FROM doc_embeddings")
        total_vectors = cursor.fetchone().get("total", 0)
        
        if total_vectors == 0:
            raise HTTPException(
                status_code=404,
                detail="No vectors found in database"
            )
        
        # Get dimension (from first vector)
        cursor.execute("SELECT VEC_Dimensions(embedding) as dim FROM doc_embeddings LIMIT 1")
        dimension = cursor.fetchone().get("dim", 384)
        
        # Sample vectors for distribution analysis
        sample_size = min(request.sample_size, total_vectors)
        
        cursor.execute(f"""
            SELECT embedding 
            FROM doc_embeddings 
            ORDER BY RAND() 
            LIMIT {sample_size}
        """)
        
        samples = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        # Calculate pairwise distances (sample)
        distances = []
        for i in range(min(100, len(samples))):
            for j in range(i + 1, min(100, len(samples))):
                # Simplified distance calculation
                # In production, use actual vector distance
                distances.append(0.5)  # Placeholder
        
        # Calculate statistics
        if distances:
            statistics_dict = {
                "mean_distance": round(statistics.mean(distances), 3),
                "median_distance": round(statistics.median(distances), 3),
                "std_dev": round(statistics.stdev(distances), 3) if len(distances) > 1 else 0.0,
                "min_distance": round(min(distances), 3),
                "max_distance": round(max(distances), 3)
            }
            
            # Recommend threshold based on median + 1 std dev
            recommended_threshold = round(
                statistics_dict["median_distance"] + statistics_dict["std_dev"],
                3
            )
            recommended_threshold = min(recommended_threshold, 0.9)
            
            # Recommend limit based on distribution
            recommended_limit = 10 if total_vectors < 1000 else 20
        else:
            statistics_dict = {
                "mean_distance": 0.5,
                "median_distance": 0.5,
                "std_dev": 0.1,
                "min_distance": 0.0,
                "max_distance": 1.0
            }
            recommended_threshold = 0.7
            recommended_limit = 10
        
        # Update cache
        _distribution_cache["last_update"] = time.time()
        _distribution_cache["statistics"] = statistics_dict
        _distribution_cache["recommended_threshold"] = recommended_threshold
        _distribution_cache["recommended_limit"] = recommended_limit
        
        return VectorDistributionResponse(
            success=True,
            total_vectors=total_vectors,
            dimension=dimension,
            statistics=statistics_dict,
            recommended_threshold=recommended_threshold,
            recommended_limit=recommended_limit
        )
        
    except Exception as e:
        service_error = ErrorFactory.service_error(
            "Vector Distribution Analyzer",
            f"Failed to analyze vector distribution for database {request.database}",
            original_error=e
        )
        raise HTTPException(
            status_code=500,
            detail=str(service_error)
        )


@router.get("/vector/cache-stats")
async def get_cache_statistics():
    """
    üìà Get Cached Distribution Statistics
    
    Returns cached distribution statistics without re-analyzing
    """
    
    if not _distribution_cache.get("last_update"):
        return {
            "success": False,
            "message": "No cached statistics available. Run /vector/analyze-distribution first."
        }
    
    return {
        "success": True,
        "last_update": _distribution_cache["last_update"],
        "statistics": _distribution_cache["statistics"],
        "recommended_threshold": _distribution_cache["recommended_threshold"],
        "recommended_limit": _distribution_cache["recommended_limit"]
    }
